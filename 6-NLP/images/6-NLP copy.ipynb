{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vomJzxlCL4yM"
      },
      "source": [
        "---\n",
        "6 Natural Language Processing (NLP)\n",
        "---\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/SMC-AAU-CPH/ML-For-Beginners/blob/main/6-NLP/images/6-NLP.ipynb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqYNwDJoM_JC",
        "outputId": "20ef22c6-dd91-49f6-827e-a028db1fa4f9"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "  # !mkdir -p data\n",
        "  # !wget https://raw.githubusercontent.com/SMC-AAU-CPH/med4-ap-jupyter/main/lecture7_Fourer_Transfom/data/trumpet.wav -P data\n",
        "  # !curl https://raw.githubusercontent.com/cs109/2015lab8/master/data/pride_and_prejudice.txt -o pride.txt\n",
        "  !pip install pandas==1.5.3 pycaret==2.0.0 textblob\n",
        "except:\n",
        "  IN_COLAB = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! pip install pycaret==2.0.0 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZB8g82FP83f",
        "outputId": "32549c32-fae4-44bb-8ca4-70727a3a8e5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.0\n"
          ]
        }
      ],
      "source": [
        "# check version\n",
        "from pycaret.utils import version\n",
        "version()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCroLNd_PTQB"
      },
      "source": [
        "# 6.1 Introduction\n",
        "Where we learn about Elisa, Turing Test, and Computational Lingustics, then build a (random) conversational bot.\n",
        "\n",
        "## 6.1.1 The plan\n",
        "\n",
        "Your steps when building a conversational bot:\n",
        "\n",
        "1. Print instructions advising the user how to interact with the bot\n",
        "2. Start a loop\n",
        "    * Accept user input\n",
        "    * If user has asked to exit, then exit\n",
        "    * Process user input and determine response (in this case, the response is a random choice from a list of possible generic responses)\n",
        "    * Print response\n",
        "3. loop back to step 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvjcAphqL4yO",
        "outputId": "ec4e6f1c-1f00-4f98-c756-43b327915775"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello, I am Marvin, the simple robot.\n",
            "You can end this conversation at any time by typing 'bye'\n",
            "After typing each answer, press 'enter'\n",
            "How are you today?\n",
            "Let's change the subject.\n",
            "Why do you say that?\n",
            "Why do you say that?\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "# This list contains the random responses (you can add your own or translate them into your own language too)\n",
        "random_responses = [\"That is quite interesting, please tell me more.\",\n",
        "                    \"I see. Do go on.\",\n",
        "                    \"Why do you say that?\",\n",
        "                    \"Funny weather we've been having, isn't it?\",\n",
        "                    \"Let's change the subject.\",\n",
        "                    \"Did you catch the game last night?\"]\n",
        "\n",
        "print(\"Hello, I am Marvin, the simple robot.\")\n",
        "print(\"You can end this conversation at any time by typing 'bye'\")\n",
        "print(\"After typing each answer, press 'enter'\")\n",
        "print(\"How are you today?\")\n",
        "\n",
        "while True:\n",
        "    # wait for the user to enter some text\n",
        "    user_input = input(\"> \")\n",
        "    if user_input.lower() == \"bye\":\n",
        "        # if they typed in 'bye' (or even BYE, ByE, byE etc.), break out of the loop\n",
        "        break\n",
        "    else:\n",
        "        response = random.choices(random_responses)[0]\n",
        "    print(response)\n",
        "\n",
        "print(\"It was nice talking to you, goodbye!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQCBDYylL4yP"
      },
      "source": [
        "✅ Stop and consider\n",
        "\n",
        "* Do you think the random responses would 'trick' someone into thinking that the bot actually understood them?\n",
        "* What features would the bot need to be more effective?\n",
        "* If a bot could really 'understand' the meaning of a sentence, would it need to 'remember' the meaning of previous sentences in a conversation too?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2Lohr4AL4yQ"
      },
      "source": [
        "# 6.2 [Common NLP tasks](https://paperswithcode.com/area/natural-language-processing) and techniques\n",
        "Where we learn about **tokenization** (splitting text into *tokens*), Embeddings (convert text to numerical data so that similar / related words cluster), Parsing & Part of speech tagging (e.g., as a noun, verb, or adjective), word and phrase frequencies, n-grams (A text can be split into sequences of words of a set length $n$), Noun phrase Extraction, [Sentiment analysis](https://paperswithcode.com/task/sentiment-analysis), inflection, and lemmatization (rooting of words). There are useful databases like WordNet, [GLUE and SST](https://paperswithcode.com/datasets?task=sentiment-analysis) and frameworks from classical NLP (e.g., TextBlob) or Deep Learning (e.g., [Transormers](https://huggingface.co/learn/nlp-course)).\n",
        "\n",
        "💡Take a look at a [deep learning embeddings on TensorFlow (and recall PCA and T-SNE)](https://projector.tensorflow.org)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsCANW5SL4yQ"
      },
      "source": [
        "### 6.2.1 Exercise  - using `TextBlob` library\n",
        "\n",
        "Let's use a library called TextBlob as it contains helpful APIs for tackling these types of tasks. TextBlob \"stands on the giant shoulders of [NLTK](https://nltk.org) and [pattern](https://github.com/clips/pattern), and plays nicely with both.\" It has a considerable amount of ML embedded in its API.\n",
        "\n",
        "> Note: A useful [Quick Start](https://textblob.readthedocs.io/en/dev/quickstart.html#quickstart) guide is available for TextBlob that is recommended for experienced Python developers\n",
        "\n",
        "When attempting to identify *noun phrases*, TextBlob offers several options of extractors to find noun phrases.\n",
        "\n",
        "1. Take a look at `ConllExtractor`.\n",
        "\n",
        "   ```python\n",
        "   from textblob import TextBlob\n",
        "   from textblob.np_extractors import ConllExtractor\n",
        "   # import and create a Conll extractor to use later\n",
        "   extractor = ConllExtractor()\n",
        "\n",
        "   # later when you need a noun phrase extractor:\n",
        "   user_input = input(\"> \")\n",
        "   user_input_blob = TextBlob(user_input, np_extractor=extractor)  # note non-default extractor specified\n",
        "   np = user_input_blob.noun_phrases                                  \n",
        "   ```\n",
        "   > What's going on here? [ConllExtractor](https://textblob.readthedocs.io/en/dev/api_reference.html?highlight=Conll#textblob.en.np_extractors.ConllExtractor) is \"A noun phrase extractor that uses chunk parsing trained with the ConLL-2000 training corpus.\" ConLL-2000 refers to the 2000 Conference on Computational Natural Language Learning. Each year the conference hosted a workshop to tackle a thorny NLP problem, and in 2000 it was noun chunking. A model was trained on the Wall Street Journal, with \"sections 15-18 as training data (211727 tokens) and section 20 as test data (47377 tokens)\". You can look at the procedures used [here](https://www.clips.uantwerpen.be/conll2000/chunking/) and the [results](https://ifarm.nl/erikt/research/np-chunking.html).\n",
        "   >"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfFABUBwL4yQ"
      },
      "source": [
        "### 6.2.2 Challenge - improving our bot with NLP\n",
        "\n",
        "In 6.1 we built a very simple Q&A bot. Now, we'll make Marvin a bit more sympathetic by analyzing your input for sentiment and printing out a response to match the sentiment. We'll also need to identify a `noun_phrase` and ask about it.\n",
        "\n",
        "Our steps when building a better conversational bot:\n",
        "\n",
        "1. Print instructions advising the user how to interact with the bot\n",
        "2. Start loop\n",
        "   1. Accept user input\n",
        "   2. If user has asked to exit, then exit\n",
        "   3. Process user input and determine appropriate sentiment response\n",
        "   4. If a noun phrase is detected in the sentiment, pluralize it and ask for more input on that topic\n",
        "   5. Print response\n",
        "3. loop back to step 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpFc3nGWL4yR",
        "outputId": "38bde2f7-1b84-411f-8ecb-b10bbc485700"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from textblob import TextBlob\n",
        "from textblob.np_extractors import ConllExtractor\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "extractor = ConllExtractor()\n",
        "\n",
        "\n",
        "nltk.download('conll2000')\n",
        "\n",
        "print(\"Hello, I am Marvin, the simple robot.\")\n",
        "print(\"You can end this conversation at any time by typing 'bye'\")\n",
        "print(\"After typing each answer, press 'enter'\")\n",
        "print(\"How are you today?\")\n",
        "\n",
        "while True:\n",
        "        # wait for the user to enter some text\n",
        "        user_input = input(\"> \")\n",
        "\n",
        "        if user_input.lower() == \"bye\":\n",
        "            # if they typed in 'bye' (or even BYE, ByE, byE etc.), break out of the loop\n",
        "            break\n",
        "        else:\n",
        "            # Create a TextBlob based on the user input. Then extract the noun phrases\n",
        "            user_input_blob = TextBlob(user_input, np_extractor=extractor)\n",
        "            np = user_input_blob.noun_phrases\n",
        "            response = \"\"\n",
        "            if user_input_blob.polarity <= -0.5:\n",
        "                response = \"Oh dear, that sounds bad. \"\n",
        "            elif user_input_blob.polarity <= 0:\n",
        "                response = \"Hmm, that's not great. \"\n",
        "            elif user_input_blob.polarity <= 0.5:\n",
        "                response = \"Well, that sounds positive. \"\n",
        "            elif user_input_blob.polarity <= 1:\n",
        "                response = \"Wow, that sounds great. \"\n",
        "\n",
        "            if len(np) != 0:\n",
        "                # There was at least one noun phrase detected, so ask about that and pluralise it\n",
        "                # e.g. cat -> cats or mouse -> mice\n",
        "                response = response + \"Can you tell me more about \" + np[0].pluralize() + \"?\"\n",
        "            else:\n",
        "                response = response + \"Can you tell me more?\"\n",
        "            print(response)\n",
        "\n",
        "print(\"It was nice talking to you, goodbye!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sLRpRT9L4yR"
      },
      "source": [
        "# 6.3 Translation and sentiment analysis with ML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwbsDAIUL4yR"
      },
      "source": [
        "### 6.3.1 Translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFZ4ol3qL4yR",
        "outputId": "569a61d7-39c8-4ef0-910b-253ed3d96a6d"
      },
      "outputs": [],
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "blob = TextBlob(\n",
        "    \"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife!\"\n",
        ")\n",
        "blob.translate(from_lang=\"en\",to=\"da\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-OjMWiWL4yS"
      },
      "source": [
        "> What's going on here? and why is TextBlob so good at translation? Well, behind the scenes, it's using Google translate, a sophisticated AI able to parse millions of phrases to predict the best strings for the task at hand. There's nothing manual going on here and you need an internet connection to use `blob.translate`.\n",
        "\n",
        "✅ Try some more sentences. Which is better, ML or human translation? In which cases?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13cZNCgyL4yS"
      },
      "source": [
        "### 6.3.2 Sentiment\n",
        "Sentiment is measured in with a *polarity* of -1 to 1, meaning -1 is the most negative sentiment, and 1 is the most positive.\n",
        "\n",
        "Sentiment is also measured with an 0 - 1 score for objectivity (0) and subjectivity (1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6s298B_FL4yS",
        "outputId": "29b68ba9-bcdb-4f11-cdc1-effdc0035885"
      },
      "outputs": [],
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "quote1 = \"\"\"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.\"\"\"\n",
        "\n",
        "quote2 = \"\"\"Darcy, as well as Elizabeth, really loved them; and they were both ever sensible of the warmest gratitude towards the persons who, by bringing her into Derbyshire, had been the means of uniting them.\"\"\"\n",
        "\n",
        "sentiment1 = TextBlob(quote1).sentiment\n",
        "sentiment2 = TextBlob(quote2).sentiment\n",
        "\n",
        "print(quote1 + \" has a sentiment of \" + str(sentiment1))\n",
        "print(quote2 + \" has a sentiment of \" + str(sentiment2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKMJQoXwL4yS"
      },
      "source": [
        "#### Challenge - check sentiment polarity\n",
        "Our task is to determine, using sentiment polarity, if *Pride and Prejudice* has more absolutely positive sentences than absolutely negative ones. For this task, you may assume that a polarity score of 1 or -1 is absolutely positive or negative respectively.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. ~~Download a [copy of Pride and Prejudice](https://www.gutenberg.org/files/1342/1342-h/1342-h.htm) from Project Gutenberg as a .txt file. Remove the metadata at the start and end of the file, leaving only the original text~~ Cumhur used `curl https://raw.githubusercontent.com/cs109/2015lab8/master/data/pride_and_prejudice.txt -o pride.txt` and did not clean up (as metadata is neutral).\n",
        "2. Open the file in Python and extract the contents as a string\n",
        "3. Create a TextBlob using the book string\n",
        "4. Analyse each sentence in the book in a loop\n",
        "   1. If the polarity is 1 or -1 store the sentence in an array or list of positive or negative messages\n",
        "5. At the end, print out all the positive sentences and negative sentences (separately) and the number of each."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-OUYqP2L4yS"
      },
      "outputs": [],
      "source": [
        "from textblob import TextBlob\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QC4NiiufL4yS"
      },
      "outputs": [],
      "source": [
        "# You should download the book text, clean it, and import it here\n",
        "with open(\"pride.txt\", encoding=\"utf8\") as f:\n",
        "    file_contents = f.read()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ej8z_t0CL4yS"
      },
      "outputs": [],
      "source": [
        "book_pride = TextBlob(file_contents)\n",
        "positive_sentiment_sentences = []\n",
        "negative_sentiment_sentences = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNK1smxLL4yS"
      },
      "outputs": [],
      "source": [
        "for sentence in book_pride.sentences:\n",
        "    if sentence.sentiment.polarity == 1:\n",
        "        positive_sentiment_sentences.append(sentence)\n",
        "    if sentence.sentiment.polarity == -1:\n",
        "        negative_sentiment_sentences.append(sentence)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OvcBxdyL4yS",
        "outputId": "bb4c729c-c8a8-49d1-c05b-5a88f49d09b8"
      },
      "outputs": [],
      "source": [
        "print(\"The \" + str(len(positive_sentiment_sentences)) + \" most positive sentences:\")\n",
        "for sentence in positive_sentiment_sentences:\n",
        "    print(\"+ \" + str(sentence.replace(\"\\n\", \"\").replace(\"      \", \" \")))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXnWKelGL4yS",
        "outputId": "853e90df-4f9a-461e-a3c7-39b5ad60ff8f"
      },
      "outputs": [],
      "source": [
        "print(\"The \" + str(len(negative_sentiment_sentences)) + \" most negative sentences:\")\n",
        "for sentence in negative_sentiment_sentences:\n",
        "    print(\"- \" + str(sentence.replace(\"\\n\", \"\").replace(\"      \", \" \")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIVmJt8-L4yT"
      },
      "source": [
        "✅ Knowledge Check: Consult to the [3-Translation-Sentiment/README.md](../3-Translation-Sentiment/README.md)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTIzqIuJL4yT"
      },
      "source": [
        "# 6.4 Hotel Reviews 1\n",
        "\n",
        "See [4-Hotel-Reviews-1/README.md](../4-Hotel-Reviews-1/README.md) for details and the questions we ask.\n",
        "\n",
        "❗️If you don't like downloading from Kaggle or elsewhere as we do, you could try if you have huggingface datasets installed:\n",
        " ```python\n",
        " from datasets import load_dataset\n",
        " dataset = load_dataset(\"ashraq/hotel-reviews\")\n",
        " df = dataset.to_pandas()\n",
        " df.head() # note the data HF has only 3 columns and already cleaned\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuZyBkFOL4yT"
      },
      "outputs": [],
      "source": [
        "## EDA\n",
        "import pandas as pd\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAq9OiLLL4yT"
      },
      "outputs": [],
      "source": [
        "def get_difference_review_avg(row):\n",
        "    return row[\"Average_Score\"] - row[\"Calc_Average_Score\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuhuGIU6L4yT",
        "outputId": "56a31ccc-2484-441c-c627-f887f70ba64e"
      },
      "outputs": [],
      "source": [
        "# Load the hotel reviews from CSV\n",
        "print(\"Loading data file now, this could take a while depending on file size\")\n",
        "start = time.time()\n",
        "df = pd.read_csv('Hotel_Reviews.csv')\n",
        "end = time.time()\n",
        "print(\"Loading took \" + str(round(end - start, 2)) + \" seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1cT3oniL4yT",
        "outputId": "9d27fa9f-3856-45e0-f35c-b6ee745ff108"
      },
      "outputs": [],
      "source": [
        "# What shape is the data (rows, columns)?\n",
        "print(\"The shape of the data (rows, cols) is \" + str(df.shape))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 695
        },
        "id": "yvYmh4Ohb-H1",
        "outputId": "d61266ee-0ae8-48f4-85be-b9c97f597324"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvR4zM5wL4yT"
      },
      "outputs": [],
      "source": [
        "# value_counts() creates a Series object that has index and values\n",
        "#                in this case, the country and the frequency they occur in reviewer nationality\n",
        "nationality_freq = df[\"Reviewer_Nationality\"].value_counts()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKzRwHRxL4yT",
        "outputId": "1b770bcf-01a6-458a-99bf-3eda8acdd180"
      },
      "outputs": [],
      "source": [
        "# What reviewer nationality is the most common in the dataset?\n",
        "print(\"The highest frequency reviewer nationality is \" + str(nationality_freq.index[0]).strip() + \" with \" + str(nationality_freq[0]) + \" reviews.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnAIP3zaL4yT",
        "outputId": "ff7469b2-858b-40bd-aa7f-f0da81ff5a95"
      },
      "outputs": [],
      "source": [
        "# What is the top 10 most common nationalities and their frequencies?\n",
        "print(\"The top 10 highest frequency reviewer nationalities are:\")\n",
        "print(nationality_freq[0:10].to_string())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Uqc31gtL4yT",
        "outputId": "da5c727b-6f67-4c6c-cdb7-06bc1e17ecb5"
      },
      "outputs": [],
      "source": [
        "# How many unique nationalities are there?\n",
        "print(\"There are \" + str(nationality_freq.index.size) + \" unique nationalities in the dataset\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vQuZ4ECL4yT",
        "outputId": "b03e7cde-76d6-41b2-cf32-111c08363ab9"
      },
      "outputs": [],
      "source": [
        "# What was the most frequently reviewed hotel for the top 10 nationalities - print the hotel and number of reviews\n",
        "for nat in nationality_freq[:10].index:\n",
        "   # First, extract all the rows that match the criteria into a new dataframe\n",
        "   nat_df = df[df[\"Reviewer_Nationality\"] == nat]\n",
        "   # Now get the hotel freq\n",
        "   freq = nat_df[\"Hotel_Name\"].value_counts()\n",
        "   print(\"The most reviewed hotel for \" + str(nat).strip() + \" was \" + str(freq.index[0]) + \" with \" + str(freq[0]) + \" reviews.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbIoWcuLL4yT"
      },
      "outputs": [],
      "source": [
        "# How many reviews are there per hotel (frequency count of hotel) and do the results match the value in `Total_Number_of_Reviews`?\n",
        "# First create a new dataframe based on the old one, removing the uneeded columns\n",
        "hotel_freq_df = df.drop([\"Hotel_Address\", \"Additional_Number_of_Scoring\", \"Review_Date\", \"Average_Score\", \"Reviewer_Nationality\", \"Negative_Review\", \"Review_Total_Negative_Word_Counts\", \"Positive_Review\", \"Review_Total_Positive_Word_Counts\", \"Total_Number_of_Reviews_Reviewer_Has_Given\", \"Reviewer_Score\", \"Tags\", \"days_since_review\", \"lat\", \"lng\"], axis = 1)\n",
        "# Group the rows by Hotel_Name, count them and put the result in a new column Total_Reviews_Found\n",
        "hotel_freq_df['Total_Reviews_Found'] = hotel_freq_df.groupby('Hotel_Name').transform('count')\n",
        "# Get rid of all the duplicated rows\n",
        "hotel_freq_df = hotel_freq_df.drop_duplicates(subset = [\"Hotel_Name\"])\n",
        "# print()\n",
        "# print(hotel_freq_df.to_string())\n",
        "# print(str(hotel_freq_df.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1WsRMEdL4yT",
        "outputId": "55989cfa-585a-44b4-e46c-d74569cd065c"
      },
      "outputs": [],
      "source": [
        "# While there is an `Average_Score` for each hotel according to the dataset,\n",
        "# you can also calculate an average score (getting the average of all reviewer scores in the dataset for each hotel)\n",
        "# Add a new column to your dataframe with the column header `Calc_Average_Score` that contains that calculated average.\n",
        "df['Calc_Average_Score'] = round(df.groupby('Hotel_Name').Reviewer_Score.transform('mean'), 1)\n",
        "# Add a new column with the difference between the two average scores\n",
        "df[\"Average_Score_Difference\"] = df.apply(get_difference_review_avg, axis = 1)\n",
        "# Create a df without all the duplicates of Hotel_Name (so only 1 row per hotel)\n",
        "review_scores_df = df.drop_duplicates(subset = [\"Hotel_Name\"])\n",
        "# Sort the dataframe to find the lowest and highest average score difference\n",
        "review_scores_df = review_scores_df.sort_values(by=[\"Average_Score_Difference\"])\n",
        "print(review_scores_df[[\"Average_Score_Difference\", \"Average_Score\", \"Calc_Average_Score\", \"Hotel_Name\"]])\n",
        "# Do any hotels have the same (rounded to 1 decimal place) `Average_Score` and `Calc_Average_Score`?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFYEihQ4L4yT"
      },
      "source": [
        "💡As we proceed to NLP, take this opportunity to read through the '[NLTK book](https://www.nltk.org/book/)' and try out its exercises."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnhcRLTKL4yT"
      },
      "source": [
        "# 6.5-Hotel-Reviews-2: Sentiment Analysis\n",
        "\n",
        "Now that you have explored the dataset in detail, it's time to filter the columns and then use NLP techniques on the dataset to gain new insights about the hotels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Enbqlm86L4yU"
      },
      "source": [
        "### Exercise: a bit more data processing\n",
        "\n",
        "Clean the data just a bit more. Add columns that will be useful later, change the values in other columns, and drop certain columns completely."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B36iB40cL4yU"
      },
      "outputs": [],
      "source": [
        "import ast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZL8ELpOL4yU"
      },
      "outputs": [],
      "source": [
        "def replace_address(row):\n",
        "    if \"Netherlands\" in row[\"Hotel_Address\"]:\n",
        "        return \"Amsterdam, Netherlands\"\n",
        "    elif \"Barcelona\" in row[\"Hotel_Address\"]:\n",
        "        return \"Barcelona, Spain\"\n",
        "    elif \"United Kingdom\" in row[\"Hotel_Address\"]:\n",
        "        return \"London, United Kingdom\"\n",
        "    elif \"Milan\" in row[\"Hotel_Address\"]:\n",
        "        return \"Milan, Italy\"\n",
        "    elif \"France\" in row[\"Hotel_Address\"]:\n",
        "        return \"Paris, France\"\n",
        "    elif \"Vienna\" in row[\"Hotel_Address\"]:\n",
        "        return \"Vienna, Austria\"\n",
        "    else:\n",
        "        return row.Hotel_Address\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4X5tN6RL4yU"
      },
      "outputs": [],
      "source": [
        "# dropping columns we will not use:\n",
        "df.drop([\"lat\", \"lng\"], axis = 1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixaFNvlJL4yY"
      },
      "outputs": [],
      "source": [
        "# Replace all the addresses with a shortened, more useful form\n",
        "df[\"Hotel_Address\"] = df.apply(replace_address, axis = 1)\n",
        "# Drop `Additional_Number_of_Scoring`\n",
        "df.drop([\"Additional_Number_of_Scoring\"], axis = 1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHsjqtMGL4yY"
      },
      "outputs": [],
      "source": [
        "# Replace `Total_Number_of_Reviews` and `Average_Score` with our own calculated values\n",
        "df.Average_Score = round(df.groupby('Hotel_Name').Reviewer_Score.transform('mean'), 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Oz7kPUyL4yY"
      },
      "outputs": [],
      "source": [
        "# Process the Tags into new columns\n",
        "# The file Hotel_Reviews_Tags.py, identifies the most important tags\n",
        "# Leisure trip, Couple, Solo traveler, Business trip, Group combined with Travelers with friends,\n",
        "# Family with young children, Family with older children, With a pet\n",
        "\n",
        "df[\"Leisure_trip\"] = df.Tags.apply(lambda tag: 1 if \"Leisure trip\" in tag else 0)\n",
        "df[\"Couple\"] = df.Tags.apply(lambda tag: 1 if \"Couple\" in tag else 0)\n",
        "df[\"Solo_traveler\"] = df.Tags.apply(lambda tag: 1 if \"Solo traveler\" in tag else 0)\n",
        "df[\"Business_trip\"] = df.Tags.apply(lambda tag: 1 if \"Business trip\" in tag else 0)\n",
        "df[\"Group\"] = df.Tags.apply(lambda tag: 1 if \"Group\" in tag or \"Travelers with friends\" in tag else 0)\n",
        "df[\"Family_with_young_children\"] = df.Tags.apply(lambda tag: 1 if \"Family with young children\" in tag else 0)\n",
        "df[\"Family_with_older_children\"] = df.Tags.apply(lambda tag: 1 if \"Family with older children\" in tag else 0)\n",
        "df[\"With_a_pet\"] = df.Tags.apply(lambda tag: 1 if \"With a pet\" in tag else 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSC-1WI8L4yY"
      },
      "outputs": [],
      "source": [
        "# No longer need any of these columns\n",
        "df.drop([\"Review_Date\", \"Review_Total_Negative_Word_Counts\", \"Review_Total_Positive_Word_Counts\", \"days_since_review\", \"Total_Number_of_Reviews_Reviewer_Has_Given\"], axis = 1, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-dD4VZ4L4yY",
        "outputId": "3fcf3221-0fbb-4cb8-c857-9ca467f4785a"
      },
      "outputs": [],
      "source": [
        "# Saving new data file with calculated columns\n",
        "print(\"Saving results to Hotel_Reviews_Filtered.csv\")\n",
        "df.to_csv(r'Hotel_Reviews_Filtered.csv', index = False)\n",
        "end = time.time()\n",
        "print(\"Filtering took \" + str(round(end - start, 2)) + \" seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3qbusuUL4yY"
      },
      "source": [
        "### Exercise: load and save the filtered data for sentiment analysis\n",
        "The code skeleton should like this (assuming) independent execution:\n",
        "```python\n",
        "import time\n",
        "import pandas as pd\n",
        "import nltk as nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Load the filtered hotel reviews from CSV\n",
        "df = pd.read_csv('Hotel_Reviews_Filtered.csv')\n",
        "\n",
        "# You code will be added here\n",
        "\n",
        "\n",
        "# Finally remember to save the hotel reviews with new NLP data added\n",
        "print(\"Saving results to Hotel_Reviews_NLP.csv\")\n",
        "df.to_csv(r'Hotel_Reviews_NLP.csv', index = False)\n",
        "```\n",
        "\n",
        "The actual code is below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAwg3QmNL4yY",
        "outputId": "fdb4e7bc-4971-4874-f87d-78bf5af76ba9"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_s-q6UwjL4yY"
      },
      "outputs": [],
      "source": [
        "vader_sentiment = SentimentIntensityAnalyzer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDA1htYCL4yY"
      },
      "outputs": [],
      "source": [
        "# There are 3 possibilities of input for a review:\n",
        "# It could be \"No Negative\", in which case, return 0\n",
        "# It could be \"No Positive\", in which case, return 0\n",
        "# It could be a review, in which case calculate the sentiment\n",
        "def calc_sentiment(review):\n",
        "    if review == \"No Negative\" or review == \"No Positive\":\n",
        "        return 0\n",
        "    return vader_sentiment.polarity_scores(review)[\"compound\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcuFwJNuL4yY"
      },
      "outputs": [],
      "source": [
        "# Load the hotel reviews from CSV (already filtered)\n",
        "df = pd.read_csv(\"Hotel_Reviews_Filtered.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iI3ZXUqAL4yY"
      },
      "outputs": [],
      "source": [
        "# Remove stop words - can be slow for a lot of text!\n",
        "# Ryan Han (ryanxjhan on Kaggle) has a great post measuring performance of different stop words removal approaches\n",
        "# https://www.kaggle.com/ryanxjhan/fast-stop-words-removal # using the approach that Ryan recommends\n",
        "start = time.time()\n",
        "cache = set(stopwords.words(\"english\"))\n",
        "def remove_stopwords(review):\n",
        "    text = \" \".join([word for word in review.split() if word not in cache])\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxFn_oCQL4yZ"
      },
      "outputs": [],
      "source": [
        "# Remove the stop words from both columns\n",
        "df.Negative_Review = df.Negative_Review.apply(remove_stopwords)\n",
        "df.Positive_Review = df.Positive_Review.apply(remove_stopwords)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vUcQ7PKL4yZ",
        "outputId": "2463bdc3-a9fe-4220-f95e-e98709aa5ca2"
      },
      "outputs": [],
      "source": [
        "end = time.time()\n",
        "print(\"Removing stop words took \" + str(round(end - start, 2)) + \" seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAn4ZzzPL4yZ",
        "outputId": "d1cc83f6-282b-47cf-fe1f-09ef192980aa"
      },
      "outputs": [],
      "source": [
        "# Add a negative sentiment and positive sentiment column. (This can take a moment, ~1,5 minute)\n",
        "print(\"Calculating sentiment columns for both positive and negative reviews\")\n",
        "start = time.time()\n",
        "df[\"Negative_Sentiment\"] = df.Negative_Review.apply(calc_sentiment)\n",
        "df[\"Positive_Sentiment\"] = df.Positive_Review.apply(calc_sentiment)\n",
        "end = time.time()\n",
        "print(\"Calculating sentiment took \" + str(round(end - start, 2)) + \" seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9MIQftXL4yZ",
        "outputId": "9bd63c5a-067a-40e7-9718-8ba680ccf051"
      },
      "outputs": [],
      "source": [
        "df = df.sort_values(by=[\"Negative_Sentiment\"], ascending=True)\n",
        "print(df[[\"Negative_Review\", \"Negative_Sentiment\"]])\n",
        "df = df.sort_values(by=[\"Positive_Sentiment\"], ascending=True)\n",
        "print(df[[\"Positive_Review\", \"Positive_Sentiment\"]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXP3AA3zL4yZ"
      },
      "outputs": [],
      "source": [
        "# Reorder the columns (This is cosmetic, but to make it easier to explore the data later)\n",
        "df = df.reindex([\"Hotel_Name\", \"Hotel_Address\", \"Total_Number_of_Reviews\", \"Average_Score\", \"Reviewer_Score\", \"Negative_Sentiment\", \"Positive_Sentiment\", \"Reviewer_Nationality\", \"Leisure_trip\", \"Couple\", \"Solo_traveler\", \"Business_trip\", \"Group\", \"Family_with_young_children\", \"Family_with_older_children\", \"With_a_pet\", \"Negative_Review\", \"Positive_Review\"], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wzq_fQcaL4yZ",
        "outputId": "df0acaba-7cbb-4d0d-f2b4-3a4ac304daab"
      },
      "outputs": [],
      "source": [
        "print(\"Saving results to Hotel_Reviews_NLP.csv\")\n",
        "df.to_csv(r\"Hotel_Reviews_NLP.csv\", index = False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "MLME-23",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
